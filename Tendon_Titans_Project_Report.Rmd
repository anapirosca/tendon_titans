---
title: "Module 7 Project Report"
author: "Ana Pirosca, Jonah Kotzen, Katie Miller, Jessica Stolz"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MuMIn)
library(tidyverse)
library(readr)
library(stringr)
library(purrr)

```

## Introduction
How do muscles work when you're tired? Recognizing the fundamental role of length in muscle force generation, we will explore the impact of fatigue on the force-length relationship, particularly focusing on the upper limb's forearm flexors and their 3rd order lever system. While previous research has delved into the basics of muscle mechanics, the missing piece that makes our study relevant is a comprehensive exploration of how fatigue specifically influences the force-length relationship in the context of isometric muscle contractions, offering insights into practical scenarios like sustained muscle effort or prolonged physical activity (@enoka2008muscle).

Thus, we have two big questions: First, do the data about how much force your muscles can make match a certain pattern when you're pushing against something? Second, does the angle at which your muscles work change when you're tired compared to when you're not tired? Our investigation dives into the biomechanical aspects of muscle architecture, considering the mediation of force at the fiber level by crossbridges and the implications of lever configurations. (@chen2007intensity)

## Methods
In order to answer the questions above, we obtained experimental data by fabricating and programming a load-cell data acquisition system (via Arduino) then we recorded isometric MVCs at 12 distinct elbow angle by moving the upper body in different locations relative to the goniometer that is secured to a table. In orer to obtain fatigued data, we eccentrically lowered heavy items with the same arm used in the initial experiment, waited two minutes, then performed the isometric MVCs again. 

In terms of how the class-wide data for an isometric MVC match a typical Force-Length relationship under control and fatigued conditions, we first normalized the data in order to make every student's relative force comparable. Then, we created a polynomial model in order to predict the maximum force at a specified angle under fatigued and non-fatigued conditions. In order to understand whether the differences observed in the maximum forces of the muscles were statistically significant, we conducted an ANOVA analysis.

## Software and Coding the Arduino
The Arduino was programmed using the following sketch...


HX711 scale;

void setup() {
  Serial.begin(115200);
  scale.begin(6, 7);
  scale.tare();
}

void loop() {
  float newReading = scale.get_units(10);
  Serial.println(newReading);
}

# Jonah's Manipulation of the New_Data

## Meta Data

``` {r new, include=TRUE}

# Get all CSV files in the 'new_data' folder
file_paths <- list.files(path = "new_data", pattern = "\\.csv$", full.names = TRUE)

# Initialize an empty tibble to store the combined metadata
combined_metadata <- tibble(
  TeamName = character(),
  Person = character(),
  State = character(),
  Angle = double()
)

# Process each file
for (file_path in file_paths) {
  # Extract only the file name from the file path
  file_name <- basename(file_path)
  
# Extract metadata from the file name
parts <- str_match(file_name, "([a-zA-Z0-9]+)_(.+)_(control|fatigue)_(\\d+(\\.\\d+)?)\\.csv")
  
if (nrow(parts) > 0) {
  teamname <- parts[, 2]
  person <- parts[, 3]
  state <- parts[, 4]
  angle <- as.numeric(parts[, 5])  # Convert angle to numeric

    # Create a tibble with the extracted metadata
    metadata <- tibble(
      TeamName = teamname,
      Person = person,
      State = state,
      Angle = angle,
      FileName = file_name
    )
    
    # Combine with the combined_metadata tibble
    combined_metadata <- bind_rows(combined_metadata, metadata)
  }
}

# Print the combined metadata tibble
print(combined_metadata)

```



# Factoring in Force

```{r bug fix, include=TRUE}

force_data <- read.csv(file_path) %>% select(force)

```

```{r combined data, include = TRUE}

# Define the function to read and process each file
read_and_process_file <- function(file_path) {
  force_data <- read.csv(file_path) %>% select(force)
  tibble(filename = basename(file_path), force = force_data$force)
}

# Create an empty list to store the results
combined_data <- list()

# Loop through file_paths and read/append data
for (file_path in file_paths) {
  data <- read_and_process_file(file_path)
  combined_data <- append(combined_data, list(data))
}

# Combine the list of data frames into a single tibble
combined_data <- bind_rows(combined_data)

print(combined_data)

```

```{r stuff, include=TRUE}
# Get all CSV files in the 'new_data' folder
file_paths <- list.files(path = "new_data", pattern = "\\.csv$", full.names = TRUE)

# Initialize an empty list to store the combined data
combined_data_list <- list()

# Process each file
for (file_path in file_paths) {
  # Extract only the file name from the file path
  file_name <- basename(file_path)
  
  # Extract metadata from the file name
  parts <- str_match(file_name, "([a-zA-Z0-9]+)_(.+)_(control|fatigue)_(\\d+(\\.\\d+)?)\\.csv")
  
  if (nrow(parts) > 0) {
    teamname <- parts[, 2]
    person <- parts[, 3]
    state <- parts[, 4]
    angle <- as.numeric(parts[, 5])  # Convert angle to numeric
    
    # Read force_data and select the 'force' column
    force_data <- read.csv(file_path) %>% select(force)
    
    # Create a tibble with the extracted metadata
    metadata <- tibble(
      TeamName = teamname,
      Person = person,
      State = state,
      Angle = angle,
      Force = force_data$force  # Add the 'force' column
    )
    
    # Store the combined data in the list
    combined_data_list[[file_name]] <- metadata
  }
}

# Combine the list of data frames into a single data frame
combined_data <- do.call(rbind, combined_data_list)

# Print the combined data tibble
print(combined_data)


```


# Max Values - The Jonah Way
```{r max values , include=TRUE}

# Assuming 'combined_data' is your tibble with all the data
combined_data <- combined_data %>%
  # Ensure all measurement values are absolute values
  mutate(across(starts_with("Force"), abs)) %>%
  # Group by Name, State, and Angle to calculate max within each group
  group_by(Person, State, Angle) %>%
  # Calculate the max measurement for each group
  mutate(Max_Force = max(Force, na.rm = TRUE)) %>%
  # Calculate the ratio of each measurement to the max measurement

# View the modified data
print(combined_data)

```

# Max Values - The Chris Way
```{r max, include=TRUE}

combined_data_norm <- combined_data %>%
  group_by(Person, State) %>%
  mutate(normF = Max_Force / max(Max_Force, na.rm = TRUE)) %>%
  ungroup()

filtered_data <- combined_data %>%
  filter(TeamName == "armstrongg", Person == "Lauren", State == "fatigue") %>%
  group_by(Person, State) %>%
  mutate(normF = Max_Force / max(Max_Force, na.rm = TRUE)) %>%
  ungroup()

ggplot(filtered_data, aes(x = Angle, y = normF)) +
  geom_point() +  # Plot points
  labs(x = "Angle", y = "normF") +  # Label axes
  ggtitle("normF vs. Angle")  # Add a title

#### YOU NEED TO PREDICT FORCE USING THE 1000 ANGLES

```

1. Library Loading and Initial Data Grouping
```{r load, include=TRUE}

library(dplyr)
library(purrr)
library(broom) # For augment() and tidy()

# Group the data by 'Person' and 'State' and nest the data for model fitting
best_models <- combined_data %>%
  group_by(Person, State) %>%
  nest()

print(best_models)
```

2. Model Fitting
```{r model, include=TRUE}
best_models <- best_models %>%
  mutate(model_fits = map(data, ~ {
    data <- .x
    num_unique_points <- n_distinct(data$Angle)
    # Initialize an empty list to store models and AICc values
    models_list <- vector("list", 3)
    names(models_list) <- c("model2", "model3", "model4")
    aicc_list <- numeric(3)
    names(aicc_list) <- c("aicc2", "aicc3", "aicc4")
    # Fit models only if enough unique points are available
    if (num_unique_points > 1) {
      for (i in 2:min(4, num_unique_points - 1)) {
        model <- lm(normF ~ poly(Angle, i), data = data)
        models_list[[i - 1]] <- model
        aicc_list[i - 1] <- AICc(model)
      }
    }
    tibble(models_list, aicc_list)
  }))

##best_models$model_fits to check
```

3. Selecting the Best Model
```{r select, include=TRUE}
best_models <- best_models %>%
  mutate(best_model = map(model_fits, ~ {
    aicc_values <- .x %>% select(starts_with("aicc")) %>% unlist()
    model_names <- names(aicc_values)
    valid_aicc_values <- na.omit(aicc_values)
    if (length(valid_aicc_values) > 0) {
      best_model_index <- which.min(valid_aicc_values)
      best_model_name <- model_names[best_model_index]
      list(best_model_name = best_model_name, best_model = .x[[best_model_name]])
    } else {
      list(best_model_name = NA, best_model = NA)
    }
  })) %>%
  select(Person, State, best_model)

best_models$best_model %>% 
  map(~ .x$best_model_name) %>% 
  print()

# This will print the best_model_name for each model in the best_models list-column
print(sapply(best_models$best_model, function(x) x$best_model_name))

# Extract the best_model_name and count the occurrences
model_names <- sapply(best_models$best_model, function(x) x$best_model_name)
model_counts <- table(model_names)

# Print the counts for aicc_list.aicc2 and aicc_list.aicc4
ModelTally <- c(model_counts["aicc_list.aicc2"], model_counts["aicc_list.aicc3"], model_counts["aicc_list.aicc4"])

print(ModelTally)

#Thus, aicc_list.aicc2 / poly2 is the go-to!!

```


4. Making Predictions #ERROR BUT WORK ON PREDICTIONS UP TOP BEFORE COMING HERE# #IN ESSENCE, WE NEED TO RUN PREDICTIONS ON EVERY SINGLE STATE (CONTROL or FATIGUE)
```{r predict, include=TRUE}


library(dplyr)
library(purrr)
library(ggplot2)
library(broom)  # For augment() function

# Assuming combined_data has been properly created and contains 'Person', 'State', 'Angle', and 'normF' columns

# Step 1: Define the sequence of angles for prediction
x.pred <- seq(45, 157.5, length.out = 1000)

library(dplyr)
library(purrr)

# Assuming combined_data has been properly created and contains 'Person', 'State', 'Angle', and 'normF' columns

# Step 1: Define the sequence of angles for prediction
x.pred <- seq(45, 157.5, length.out = 1000)

# Step 2: Check the structure of .x and fit the model if it is indeed a dataframe
predictions <- best_models %>%
  mutate(predicted_values = map(data, ~ {
    # Check the structure of .x
    if (is.data.frame(.x)) {
      # .x is a dataframe, so we can proceed to fit the model
      model <- lm(normF ~ poly(Angle, 2), data = .x)
      # Make predictions using the fitted model
      new_data <- data.frame(Angle = x.pred)
      new_data$Predicted_normF <- predict(model, newdata = new_data)
      return(new_data)
    } else {
      # .x is not a dataframe, print .x to see what it is
      print(.x)
      stop("Data is not a dataframe.")
    }
  }))


```

5. Extracting Max Angle Values and Calculating Shifts #FIX AFTER CORRECTING PREDICTIONS
```{r extract, include=TRUE}
# Extract the angles of maximum normF for each 'Person' and 'State'
theta_max <- map(predictions, ~ if (all(is.na(.x$.fitted))) NA_real_ else .x$Angle[which.max(.x$.fitted)])

# Bind the 'theta_max' values to the 'best_models' dataframe
best_models <- best_models %>%
  mutate(theta_max = theta_max)

# Calculate shifts
shifts <- best_models %>%
  group_by(Person) %>%
  summarize(
    control_theta_max = mean(theta_max[State == "Control"], na.rm = TRUE),
    fatigue_theta_max = mean(theta_max[State == "Fatigue"], na.rm = TRUE),
    shift = fatigue_theta_max - control_theta_max,
    .groups = "drop"
  ) %>%
  summarize(
    mean_shift = mean(shift, na.rm = TRUE),
    se_shift = sd(shift, na.rm = TRUE) / sqrt(n())
  )

# Print the results
print(shifts)


```

## Results

## Discussion

## Author Contributions
Jonah - Methods 
Katie - Data collection + set up 
Ana - Methods 
Jess 
